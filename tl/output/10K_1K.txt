/Users/harrytran/anaconda3/envs/graph_python/bin/python "/Users/harrytran/OneDrive - The University of Texas at Dallas/Fall 2021/CS 7301/project/Transfer-Learning-in-Reinforcement-Learning/tl/transfer_model.py"
/Users/harrytran/anaconda3/envs/graph_python/lib/python3.8/site-packages/ale_py/roms/__init__.py:84: DeprecationWarning: Automatic importing of atari-py roms won't be supported in future releases of ale-py. Please migrate over to using `ale-import-roms` OR an ALE-supported ROM package. To make this warning disappear you can run `ale-import-roms --import-from-pkg atari_py.atari_roms`.For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management
  __all__ = _resolve_roms()
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
>>[Source] Evaluate un-trained agent:
Mean reward: -1402.6312 Num episodes: 100
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 200       |
|    ep_rew_mean     | -1.69e+03 |
| time/              |           |
|    episodes        | 4         |
|    fps             | 121       |
|    time_elapsed    | 6         |
|    total timesteps | 800       |
| train/             |           |
|    actor_loss      | 25        |
|    critic_loss     | 0.0209    |
|    learning_rate   | 0.001     |
|    n_updates       | 600       |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 200       |
|    ep_rew_mean     | -1.58e+03 |
| time/              |           |
|    episodes        | 8         |
|    fps             | 104       |
|    time_elapsed    | 15        |
|    total timesteps | 1600      |
| train/             |           |
|    actor_loss      | 51.9      |
|    critic_loss     | 0.04      |
|    learning_rate   | 0.001     |
|    n_updates       | 1400      |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 200       |
|    ep_rew_mean     | -1.48e+03 |
| time/              |           |
|    episodes        | 12        |
|    fps             | 100       |
|    time_elapsed    | 23        |
|    total timesteps | 2400      |
| train/             |           |
|    actor_loss      | 76        |
|    critic_loss     | 0.15      |
|    learning_rate   | 0.001     |
|    n_updates       | 2200      |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 200       |
|    ep_rew_mean     | -1.41e+03 |
| time/              |           |
|    episodes        | 16        |
|    fps             | 98        |
|    time_elapsed    | 32        |
|    total timesteps | 3200      |
| train/             |           |
|    actor_loss      | 92.8      |
|    critic_loss     | 0.352     |
|    learning_rate   | 0.001     |
|    n_updates       | 3000      |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 200       |
|    ep_rew_mean     | -1.44e+03 |
| time/              |           |
|    episodes        | 20        |
|    fps             | 96        |
|    time_elapsed    | 41        |
|    total timesteps | 4000      |
| train/             |           |
|    actor_loss      | 103       |
|    critic_loss     | 1.68      |
|    learning_rate   | 0.001     |
|    n_updates       | 3800      |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 200       |
|    ep_rew_mean     | -1.37e+03 |
| time/              |           |
|    episodes        | 24        |
|    fps             | 96        |
|    time_elapsed    | 49        |
|    total timesteps | 4800      |
| train/             |           |
|    actor_loss      | 113       |
|    critic_loss     | 1.52      |
|    learning_rate   | 0.001     |
|    n_updates       | 4600      |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 200       |
|    ep_rew_mean     | -1.37e+03 |
| time/              |           |
|    episodes        | 28        |
|    fps             | 95        |
|    time_elapsed    | 58        |
|    total timesteps | 5600      |
| train/             |           |
|    actor_loss      | 120       |
|    critic_loss     | 2.39      |
|    learning_rate   | 0.001     |
|    n_updates       | 5400      |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 200       |
|    ep_rew_mean     | -1.22e+03 |
| time/              |           |
|    episodes        | 32        |
|    fps             | 94        |
|    time_elapsed    | 67        |
|    total timesteps | 6400      |
| train/             |           |
|    actor_loss      | 120       |
|    critic_loss     | 2.37      |
|    learning_rate   | 0.001     |
|    n_updates       | 6200      |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | -1.1e+03 |
| time/              |          |
|    episodes        | 36       |
|    fps             | 94       |
|    time_elapsed    | 76       |
|    total timesteps | 7200     |
| train/             |          |
|    actor_loss      | 116      |
|    critic_loss     | 2.79     |
|    learning_rate   | 0.001    |
|    n_updates       | 7000     |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 200       |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    episodes        | 40        |
|    fps             | 93        |
|    time_elapsed    | 85        |
|    total timesteps | 8000      |
| train/             |           |
|    actor_loss      | 108       |
|    critic_loss     | 2.91      |
|    learning_rate   | 0.001     |
|    n_updates       | 7800      |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | -929     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 93       |
|    time_elapsed    | 94       |
|    total timesteps | 8800     |
| train/             |          |
|    actor_loss      | 100      |
|    critic_loss     | 2.85     |
|    learning_rate   | 0.001    |
|    n_updates       | 8600     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | -862     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 93       |
|    time_elapsed    | 102      |
|    total timesteps | 9600     |
| train/             |          |
|    actor_loss      | 91.9     |
|    critic_loss     | 3.36     |
|    learning_rate   | 0.001    |
|    n_updates       | 9400     |
---------------------------------
>>[Source] Evaluate trained agent:
Mean reward: -154.39577 Num episodes: 100
pre saved (array([2.], dtype=float32), None)
loaded (array([2.], dtype=float32), None)
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
>>[Target] Evaluate un-trained agent using source model:
Mean reward: -982.9779 Num episodes: 100
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 200       |
|    ep_rew_mean     | -1.03e+03 |
| time/              |           |
|    episodes        | 4         |
|    fps             | 123       |
|    time_elapsed    | 6         |
|    total timesteps | 800       |
| train/             |           |
|    actor_loss      | 89.7      |
|    critic_loss     | 11        |
|    learning_rate   | 0.001     |
|    n_updates       | 10600     |
----------------------------------
>>[Target] Evaluate trained agent using source model:
Mean reward: -805.20514 Num episodes: 100
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 200       |
|    ep_rew_mean     | -1.42e+03 |
| time/              |           |
|    episodes        | 4         |
|    fps             | 122       |
|    time_elapsed    | 6         |
|    total timesteps | 800       |
| train/             |           |
|    actor_loss      | 20.1      |
|    critic_loss     | 0.107     |
|    learning_rate   | 0.001     |
|    n_updates       | 600       |
----------------------------------
>>[Target] Evaluate trained agent without TL:
Mean reward: -1409.1702 Num episodes: 100

Process finished with exit code 0
