/Users/harrytran/anaconda3/envs/graph_python/bin/python "/Users/harrytran/OneDrive - The University of Texas at Dallas/Fall 2021/CS 7301/project/Transfer-Learning-in-Reinforcement-Learning/tl/transfer_model_Acrobot.py"
/Users/harrytran/anaconda3/envs/graph_python/lib/python3.8/site-packages/ale_py/roms/__init__.py:84: DeprecationWarning: Automatic importing of atari-py roms won't be supported in future releases of ale-py. Please migrate over to using `ale-import-roms` OR an ALE-supported ROM package. To make this warning disappear you can run `ale-import-roms --import-from-pkg atari_py.atari_roms`.For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management
  __all__ = _resolve_roms()
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
>>[Source] Evaluate un-trained agent:
Mean reward: -474.13 Num episodes: 100
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 321      |
|    ep_rew_mean        | -320     |
| time/                 |          |
|    fps                | 1637     |
|    iterations         | 100      |
|    time_elapsed       | 0        |
|    total_timesteps    | 500      |
| train/                |          |
|    entropy_loss       | -1.06    |
|    explained_variance | -0.132   |
|    learning_rate      | 0.0007   |
|    n_updates          | 99       |
|    policy_loss        | -4.16    |
|    value_loss         | 18.8     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 220      |
|    ep_rew_mean        | -219     |
| time/                 |          |
|    fps                | 1623     |
|    iterations         | 200      |
|    time_elapsed       | 0        |
|    total_timesteps    | 1000     |
| train/                |          |
|    entropy_loss       | -0.966   |
|    explained_variance | 0.906    |
|    learning_rate      | 0.0007   |
|    n_updates          | 199      |
|    policy_loss        | -0.815   |
|    value_loss         | 0.857    |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 183      |
|    ep_rew_mean        | -182     |
| time/                 |          |
|    fps                | 1620     |
|    iterations         | 300      |
|    time_elapsed       | 0        |
|    total_timesteps    | 1500     |
| train/                |          |
|    entropy_loss       | -0.918   |
|    explained_variance | 0.0215   |
|    learning_rate      | 0.0007   |
|    n_updates          | 299      |
|    policy_loss        | -2.43    |
|    value_loss         | 6.65     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 161      |
|    ep_rew_mean        | -160     |
| time/                 |          |
|    fps                | 1619     |
|    iterations         | 400      |
|    time_elapsed       | 1        |
|    total_timesteps    | 2000     |
| train/                |          |
|    entropy_loss       | -0.404   |
|    explained_variance | 0.741    |
|    learning_rate      | 0.0007   |
|    n_updates          | 399      |
|    policy_loss        | -0.114   |
|    value_loss         | 1        |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 150      |
|    ep_rew_mean        | -149     |
| time/                 |          |
|    fps                | 1617     |
|    iterations         | 500      |
|    time_elapsed       | 1        |
|    total_timesteps    | 2500     |
| train/                |          |
|    entropy_loss       | -0.7     |
|    explained_variance | 0.613    |
|    learning_rate      | 0.0007   |
|    n_updates          | 499      |
|    policy_loss        | 0.0387   |
|    value_loss         | 0.435    |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 147      |
|    ep_rew_mean        | -146     |
| time/                 |          |
|    fps                | 1616     |
|    iterations         | 600      |
|    time_elapsed       | 1        |
|    total_timesteps    | 3000     |
| train/                |          |
|    entropy_loss       | -0.566   |
|    explained_variance | 0.0596   |
|    learning_rate      | 0.0007   |
|    n_updates          | 599      |
|    policy_loss        | -0.94    |
|    value_loss         | 4.92     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 151      |
|    ep_rew_mean        | -150     |
| time/                 |          |
|    fps                | 1614     |
|    iterations         | 700      |
|    time_elapsed       | 2        |
|    total_timesteps    | 3500     |
| train/                |          |
|    entropy_loss       | -0.58    |
|    explained_variance | 0.004    |
|    learning_rate      | 0.0007   |
|    n_updates          | 699      |
|    policy_loss        | -3.64    |
|    value_loss         | 4.08     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 151      |
|    ep_rew_mean        | -150     |
| time/                 |          |
|    fps                | 1615     |
|    iterations         | 800      |
|    time_elapsed       | 2        |
|    total_timesteps    | 4000     |
| train/                |          |
|    entropy_loss       | -0.684   |
|    explained_variance | -0.00453 |
|    learning_rate      | 0.0007   |
|    n_updates          | 799      |
|    policy_loss        | -1.69    |
|    value_loss         | 3.78     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 146      |
|    ep_rew_mean        | -145     |
| time/                 |          |
|    fps                | 1614     |
|    iterations         | 900      |
|    time_elapsed       | 2        |
|    total_timesteps    | 4500     |
| train/                |          |
|    entropy_loss       | -0.347   |
|    explained_variance | 0.125    |
|    learning_rate      | 0.0007   |
|    n_updates          | 899      |
|    policy_loss        | -0.267   |
|    value_loss         | 3.33     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 143      |
|    ep_rew_mean        | -142     |
| time/                 |          |
|    fps                | 1614     |
|    iterations         | 1000     |
|    time_elapsed       | 3        |
|    total_timesteps    | 5000     |
| train/                |          |
|    entropy_loss       | -0.0803  |
|    explained_variance | 0.172    |
|    learning_rate      | 0.0007   |
|    n_updates          | 999      |
|    policy_loss        | -0.0232  |
|    value_loss         | 2.92     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 143      |
|    ep_rew_mean        | -142     |
| time/                 |          |
|    fps                | 1613     |
|    iterations         | 1100     |
|    time_elapsed       | 3        |
|    total_timesteps    | 5500     |
| train/                |          |
|    entropy_loss       | -0.541   |
|    explained_variance | 0.00197  |
|    learning_rate      | 0.0007   |
|    n_updates          | 1099     |
|    policy_loss        | -0.395   |
|    value_loss         | 2.35     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 141      |
|    ep_rew_mean        | -140     |
| time/                 |          |
|    fps                | 1613     |
|    iterations         | 1200     |
|    time_elapsed       | 3        |
|    total_timesteps    | 6000     |
| train/                |          |
|    entropy_loss       | -0.516   |
|    explained_variance | 0.0171   |
|    learning_rate      | 0.0007   |
|    n_updates          | 1199     |
|    policy_loss        | -0.451   |
|    value_loss         | 1.78     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 137      |
|    ep_rew_mean        | -136     |
| time/                 |          |
|    fps                | 1613     |
|    iterations         | 1300     |
|    time_elapsed       | 4        |
|    total_timesteps    | 6500     |
| train/                |          |
|    entropy_loss       | -0.436   |
|    explained_variance | 0.00416  |
|    learning_rate      | 0.0007   |
|    n_updates          | 1299     |
|    policy_loss        | -0.241   |
|    value_loss         | 1.6      |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 131      |
|    ep_rew_mean        | -130     |
| time/                 |          |
|    fps                | 1613     |
|    iterations         | 1400     |
|    time_elapsed       | 4        |
|    total_timesteps    | 7000     |
| train/                |          |
|    entropy_loss       | -0.725   |
|    explained_variance | 0.00242  |
|    learning_rate      | 0.0007   |
|    n_updates          | 1399     |
|    policy_loss        | -1.3     |
|    value_loss         | 1.28     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 129      |
|    ep_rew_mean        | -128     |
| time/                 |          |
|    fps                | 1613     |
|    iterations         | 1500     |
|    time_elapsed       | 4        |
|    total_timesteps    | 7500     |
| train/                |          |
|    entropy_loss       | -0.757   |
|    explained_variance | 0.000324 |
|    learning_rate      | 0.0007   |
|    n_updates          | 1499     |
|    policy_loss        | -0.589   |
|    value_loss         | 0.993    |
------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 127       |
|    ep_rew_mean        | -126      |
| time/                 |           |
|    fps                | 1613      |
|    iterations         | 1600      |
|    time_elapsed       | 4         |
|    total_timesteps    | 8000      |
| train/                |           |
|    entropy_loss       | -0.633    |
|    explained_variance | -0.000136 |
|    learning_rate      | 0.0007    |
|    n_updates          | 1599      |
|    policy_loss        | -0.324    |
|    value_loss         | 0.746     |
-------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 126       |
|    ep_rew_mean        | -125      |
| time/                 |           |
|    fps                | 1613      |
|    iterations         | 1700      |
|    time_elapsed       | 5         |
|    total_timesteps    | 8500      |
| train/                |           |
|    entropy_loss       | -0.757    |
|    explained_variance | -0.000578 |
|    learning_rate      | 0.0007    |
|    n_updates          | 1699      |
|    policy_loss        | -0.279    |
|    value_loss         | 0.525     |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 124      |
|    ep_rew_mean        | -123     |
| time/                 |          |
|    fps                | 1613     |
|    iterations         | 1800     |
|    time_elapsed       | 5        |
|    total_timesteps    | 9000     |
| train/                |          |
|    entropy_loss       | -0.871   |
|    explained_variance | -0.00207 |
|    learning_rate      | 0.0007   |
|    n_updates          | 1799     |
|    policy_loss        | -0.543   |
|    value_loss         | 0.35     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 123      |
|    ep_rew_mean        | -122     |
| time/                 |          |
|    fps                | 1611     |
|    iterations         | 1900     |
|    time_elapsed       | 5        |
|    total_timesteps    | 9500     |
| train/                |          |
|    entropy_loss       | -0.761   |
|    explained_variance | 6.71e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 1899     |
|    policy_loss        | -0.288   |
|    value_loss         | 0.209    |
------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 123       |
|    ep_rew_mean        | -122      |
| time/                 |           |
|    fps                | 1609      |
|    iterations         | 2000      |
|    time_elapsed       | 6         |
|    total_timesteps    | 10000     |
| train/                |           |
|    entropy_loss       | -0.511    |
|    explained_variance | -0.000287 |
|    learning_rate      | 0.0007    |
|    n_updates          | 1999      |
|    policy_loss        | -0.0379   |
|    value_loss         | 0.102     |
-------------------------------------
>>[Source] Evaluate trained agent:
Mean reward: -109.81 Num episodes: 100
pre saved (0, None)
loaded (0, None)
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 115      |
|    ep_rew_mean        | -114     |
| time/                 |          |
|    fps                | 1635     |
|    iterations         | 100      |
|    time_elapsed       | 0        |
|    total_timesteps    | 500      |
| train/                |          |
|    entropy_loss       | -0.793   |
|    explained_variance | 0.00189  |
|    learning_rate      | 0.0007   |
|    n_updates          | 2099     |
|    policy_loss        | -0.0748  |
|    value_loss         | 0.0338   |
------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 107       |
|    ep_rew_mean        | -106      |
| time/                 |           |
|    fps                | 1602      |
|    iterations         | 200       |
|    time_elapsed       | 0         |
|    total_timesteps    | 1000      |
| train/                |           |
|    entropy_loss       | -0.609    |
|    explained_variance | -0.000161 |
|    learning_rate      | 0.0007    |
|    n_updates          | 2199      |
|    policy_loss        | -0.0416   |
|    value_loss         | 0.00399   |
-------------------------------------
>>[Target] Evaluate trained agent using source model:
Mean reward: -115.2 Num episodes: 100
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 500      |
|    ep_rew_mean        | -500     |
| time/                 |          |
|    fps                | 1662     |
|    iterations         | 100      |
|    time_elapsed       | 0        |
|    total_timesteps    | 500      |
| train/                |          |
|    entropy_loss       | -0.888   |
|    explained_variance | -2.27    |
|    learning_rate      | 0.0007   |
|    n_updates          | 99       |
|    policy_loss        | -2.2     |
|    value_loss         | 12.7     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 378      |
|    ep_rew_mean        | -377     |
| time/                 |          |
|    fps                | 1638     |
|    iterations         | 200      |
|    time_elapsed       | 0        |
|    total_timesteps    | 1000     |
| train/                |          |
|    entropy_loss       | -0.798   |
|    explained_variance | -0.0639  |
|    learning_rate      | 0.0007   |
|    n_updates          | 199      |
|    policy_loss        | -0.887   |
|    value_loss         | 7.89     |
------------------------------------
>>[Target] Evaluate trained agent without TL:
Mean reward: -192.68 Num episodes: 100
--- 30.882901191711426 seconds ---

Process finished with exit code 0
