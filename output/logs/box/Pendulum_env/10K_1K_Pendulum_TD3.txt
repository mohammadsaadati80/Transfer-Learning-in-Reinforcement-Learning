/Users/harrytran/anaconda3/envs/graph_python/bin/python "/Users/harrytran/OneDrive - The University of Texas at Dallas/Fall 2021/CS 7301/project/Transfer-Learning-in-Reinforcement-Learning/tl/transfer_model.py"
/Users/harrytran/anaconda3/envs/graph_python/lib/python3.8/site-packages/ale_py/roms/__init__.py:84: DeprecationWarning: Automatic importing of atari-py roms won't be supported in future releases of ale-py. Please migrate over to using `ale-import-roms` OR an ALE-supported ROM package. To make this warning disappear you can run `ale-import-roms --import-from-pkg atari_py.atari_roms`.For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management
  __all__ = _resolve_roms()
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
>>[Source] Evaluate un-trained agent:
Mean reward: -1175.5782 Num episodes: 100
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 200       |
|    ep_rew_mean     | -1.37e+03 |
| time/              |           |
|    episodes        | 4         |
|    fps             | 113       |
|    time_elapsed    | 7         |
|    total timesteps | 800       |
| train/             |           |
|    actor_loss      | 12        |
|    critic_loss     | 0.0662    |
|    learning_rate   | 0.001     |
|    n_updates       | 600       |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 200       |
|    ep_rew_mean     | -1.37e+03 |
| time/              |           |
|    episodes        | 8         |
|    fps             | 96        |
|    time_elapsed    | 16        |
|    total timesteps | 1600      |
| train/             |           |
|    actor_loss      | 23.3      |
|    critic_loss     | 0.042     |
|    learning_rate   | 0.001     |
|    n_updates       | 1400      |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 200       |
|    ep_rew_mean     | -1.37e+03 |
| time/              |           |
|    episodes        | 12        |
|    fps             | 91        |
|    time_elapsed    | 26        |
|    total timesteps | 2400      |
| train/             |           |
|    actor_loss      | 35.6      |
|    critic_loss     | 0.189     |
|    learning_rate   | 0.001     |
|    n_updates       | 2200      |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 200       |
|    ep_rew_mean     | -1.34e+03 |
| time/              |           |
|    episodes        | 16        |
|    fps             | 90        |
|    time_elapsed    | 35        |
|    total timesteps | 3200      |
| train/             |           |
|    actor_loss      | 47.1      |
|    critic_loss     | 0.209     |
|    learning_rate   | 0.001     |
|    n_updates       | 3000      |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 200       |
|    ep_rew_mean     | -1.24e+03 |
| time/              |           |
|    episodes        | 20        |
|    fps             | 88        |
|    time_elapsed    | 45        |
|    total timesteps | 4000      |
| train/             |           |
|    actor_loss      | 55.7      |
|    critic_loss     | 0.32      |
|    learning_rate   | 0.001     |
|    n_updates       | 3800      |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 200       |
|    ep_rew_mean     | -1.15e+03 |
| time/              |           |
|    episodes        | 24        |
|    fps             | 88        |
|    time_elapsed    | 54        |
|    total timesteps | 4800      |
| train/             |           |
|    actor_loss      | 62.8      |
|    critic_loss     | 0.487     |
|    learning_rate   | 0.001     |
|    n_updates       | 4600      |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 200       |
|    ep_rew_mean     | -1.05e+03 |
| time/              |           |
|    episodes        | 28        |
|    fps             | 87        |
|    time_elapsed    | 63        |
|    total timesteps | 5600      |
| train/             |           |
|    actor_loss      | 64.5      |
|    critic_loss     | 0.595     |
|    learning_rate   | 0.001     |
|    n_updates       | 5400      |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | -939     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 87       |
|    time_elapsed    | 73       |
|    total timesteps | 6400     |
| train/             |          |
|    actor_loss      | 66       |
|    critic_loss     | 0.829    |
|    learning_rate   | 0.001    |
|    n_updates       | 6200     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | -848     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 86       |
|    time_elapsed    | 82       |
|    total timesteps | 7200     |
| train/             |          |
|    actor_loss      | 64.8     |
|    critic_loss     | 1.13     |
|    learning_rate   | 0.001    |
|    n_updates       | 7000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | -773     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 86       |
|    time_elapsed    | 92       |
|    total timesteps | 8000     |
| train/             |          |
|    actor_loss      | 63.3     |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.001    |
|    n_updates       | 7800     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | -741     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 86       |
|    time_elapsed    | 101      |
|    total timesteps | 8800     |
| train/             |          |
|    actor_loss      | 64.8     |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.001    |
|    n_updates       | 8600     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | -690     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 86       |
|    time_elapsed    | 111      |
|    total timesteps | 9600     |
| train/             |          |
|    actor_loss      | 63.7     |
|    critic_loss     | 1.59     |
|    learning_rate   | 0.001    |
|    n_updates       | 9400     |
---------------------------------
>>[Source] Evaluate trained agent:
Mean reward: -147.90672 Num episodes: 100
pre saved (array([-1.9894387], dtype=float32), None)
loaded (array([-1.9894387], dtype=float32), None)
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
>>[Target] Evaluate un-trained agent using source model:
Mean reward: -1054.5427 Num episodes: 100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | -806     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 113      |
|    time_elapsed    | 7        |
|    total timesteps | 800      |
| train/             |          |
|    actor_loss      | 68.8     |
|    critic_loss     | 1.92     |
|    learning_rate   | 0.001    |
|    n_updates       | 10600    |
---------------------------------
>>[Target] Evaluate trained agent using source model:
Mean reward: -300.79803 Num episodes: 100
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 200       |
|    ep_rew_mean     | -1.61e+03 |
| time/              |           |
|    episodes        | 4         |
|    fps             | 112       |
|    time_elapsed    | 7         |
|    total timesteps | 800       |
| train/             |           |
|    actor_loss      | 15.2      |
|    critic_loss     | 0.125     |
|    learning_rate   | 0.001     |
|    n_updates       | 600       |
----------------------------------
>>[Target] Evaluate trained agent without TL:
Mean reward: -1634.221 Num episodes: 100
--- 159.76600408554077 seconds ---

Process finished with exit code 0
